

## Diagrama de Arquitetura ‚Äì Fluxo End-to-End

```mermaid
flowchart LR
    %% =====================
    %% PRODUCERS
    %% =====================
    subgraph PROD["Fontes / Producers (.NET em EC2)"]
        P1["Producer .NET<br/>- Extrai c√≥digo<br/>- Define key = c√≥digo<br/>- JSON ~200 bytes"]
    end

    %% =====================
    %% KAFKA / MSK
    %% =====================
    subgraph MSK["Amazon MSK (Kafka)"]
        subgraph T1["T√≥pico mt-c400"]
            P400_0["Parti√ß√£o 0<br/>(c√≥digo 300)"]
            P400_1["Parti√ß√£o 1<br/>(c√≥digo 301)"]
            P400_2["Parti√ß√£o 2<br/>(c√≥digo 302)"]
            P400_3["Parti√ß√£o 3<br/>(c√≥digo 303)"]
            P400_N["Parti√ß√µes extras<br/>(c√≥digos mais quentes)"]
        end

        subgraph T2["T√≥pico mt-c300"]
            P300_0["Parti√ß√£o 0"]
            P300_1["Parti√ß√£o 1"]
        end
    end

    %% =====================
    %% CONSUMERS
    %% =====================
    subgraph CONS["Consumers (.NET em EC2)<br/>Consumer Group"]
        C1["Consumer EC2 #1<br/>Consome 1+ parti√ß√µes"]
        C2["Consumer EC2 #2<br/>Consome 1+ parti√ß√µes"]
        C3["Consumer EC2 #N<br/>Escala horizontal"]
    end

    %% =====================
    %% LAMBDA
    %% =====================
    subgraph AWS["AWS Lambda"]
        L1["Lambda Dispatcher<br/>- Recebe lote<br/>- Roteia para servi√ßos"]
    end

    %% =====================
    %% FLUXOS
    %% =====================
    P1 -->|Produce com key=c√≥digo| MSK

    P400_0 --> C1
    P400_1 --> C2
    P400_2 --> C3
    P400_3 --> C1
    P400_N --> C2

    P300_0 --> C3
    P300_1 --> C1

    C1 -->|Batch JSON| L1
    C2 -->|Batch JSON| L1
    C3 -->|Batch JSON| L1

```
### 1Ô∏è‚É£ Producer (.NET em EC2)

* O Producer **n√£o cria parti√ß√µes dinamicamente**.
* Ele apenas:

  * L√™ a mensagem
  * Extrai o `codigo`
  * Define `key = codigo`
* O Kafka garante que **a mesma key sempre vai para a mesma parti√ß√£o**.

üëâ **Decis√£o importante**:
A l√≥gica de distribui√ß√£o est√° no **Producer**, n√£o no MSK.

---

### 2Ô∏è‚É£ T√≥picos Kafka (Dom√≠nio de Neg√≥cio)

* Cada t√≥pico representa um **dom√≠nio funcional** (`mt-c400`, `mt-c300`).
* N√£o h√° um t√≥pico por c√≥digo ‚Üí evita explos√£o de t√≥picos.
* O t√≥pico mais pesado (`mt-c400`) possui **mais parti√ß√µes**.

üëâ **Argumento forte**:

> ‚ÄúT√≥picos representam dom√≠nios, parti√ß√µes representam escala.‚Äù

---

### 3Ô∏è‚É£ Parti√ß√µes (Escala e Paralelismo)

* Cada parti√ß√£o √©:

  * Unidade de paralelismo
  * Unidade de ordena√ß√£o
* C√≥digos mais frequentes podem:

  * Ter parti√ß√µes dedicadas
  * Ou m√∫ltiplas parti√ß√µes no futuro (sub-key)

üëâ **Trade-off consciente**:
Possibilidade de hot partition, mitig√°vel com monitoramento.

---

### 4Ô∏è‚É£ Consumers em EC2 (.NET)

* Consumers fazem parte de **um √∫nico consumer group**.
* Kafka distribui automaticamente as parti√ß√µes.
* Escalar = subir mais EC2s.
* Cada consumer:

  * Consome mensagens
  * Agrupa em lote
  * Confirma offset ap√≥s sucesso

üëâ **Benef√≠cio**:
Escala horizontal sem mudan√ßa de c√≥digo.

---

### 5Ô∏è‚É£ Lambda (Fan-out e Distribui√ß√£o)

* Recebe mensagens **em lote**.
* Reduz custo de invoca√ß√£o.
* Faz roteamento para servi√ßos downstream.
* Mant√©m consumer simples e r√°pido.

üëâ **Separa√ß√£o clara de responsabilidades**:

* Kafka = throughput
* EC2 = processamento cont√≠nuo
* Lambda = distribui√ß√£o e elasticidade

---

## Mensagem-chave para convencer os pares

> ‚ÄúEssa arquitetura desloca a complexidade para onde temos controle (Producer e Consumer) e mant√©m o MSK simples, est√°vel e escal√°vel. Ela evita explos√£o de t√≥picos, permite crescimento previs√≠vel e √© compat√≠vel com volumes industriais de mensagens.‚Äù

√ìtimo ponto. Como voc√™ est√° desenhando **arquitetura de alto throughput com Kafka + MSK + .NET + Lambda**, existem **conceitos fundamentais (te√≥ricos e pr√°ticos)** que valem muito a pena voc√™ dominar e at√© **defender tecnicamente** em reuni√µes, ADRs ou banca acad√™mica. Vou organizar em **camadas conceituais**, sempre conectando com o seu cen√°rio real (13k msg/s, Pareto 80/20, m√∫ltiplos t√≥picos e parti√ß√µes).

---

## 1. Conceitos fundamentais de Kafka que voc√™ DEVE abordar

### 1.1 T√≥pico ‚â† Fila (log distribu√≠do)

**Conceito**

* Kafka n√£o √© uma fila tradicional, √© um **log distribu√≠do e particionado**.
* Mensagens **n√£o s√£o removidas ao serem consumidas**.
* Cada consumidor mant√©m seu **offset**.

**Por que isso importa no seu caso**

* Voc√™ pode ter **v√°rios consumidores lendo o mesmo t√≥pico** (ex: analytics, auditoria, distribui√ß√£o).
* O Lambda n√£o ‚Äúconsome‚Äù a mensagem, ele **processa uma posi√ß√£o do log**.
* Reprocessamento √© poss√≠vel (reset de offset).

**Frase-chave para defender**

> ‚ÄúKafka desacopla produtores e consumidores usando um log imut√°vel, o que nos d√° replay, escalabilidade e toler√¢ncia a falhas.‚Äù

---

### 1.2 Parti√ß√µes = unidade real de paralelismo

**Conceito**

* A **parti√ß√£o √© a menor unidade de paralelismo** no Kafka.
* Uma parti√ß√£o ‚Üí **1 consumidor por consumer group**.
* N√£o existe paralelismo dentro de uma parti√ß√£o.

**No seu cen√°rio**

* Se voc√™ quer processar **13.000 msg/s**, precisa dividir isso em **fatias paralelas**.
* Exemplo:

  * 13.000 / 13 parti√ß√µes ‚âà 1.000 msg/s por parti√ß√£o
  * 13 consumidores em paralelo

**Decis√£o arquitetural**

* `mt-c400` ‚Üí muitas parti√ß√µes
* `mt-c300` ‚Üí poucas parti√ß√µes

**Frase forte**

> ‚ÄúEscalar Kafka √© escalar parti√ß√µes, n√£o CPU.‚Äù

---

### 1.3 Consumer Group como mecanismo de balanceamento autom√°tico

**Conceito**

* Um **consumer group** garante:

  * Balanceamento autom√°tico de parti√ß√µes
  * Failover transparente
  * Escalabilidade horizontal

**No seu caso**

* Voc√™ sobe mais inst√¢ncias do servi√ßo C#
* Kafka redistribui parti√ß√µes
* Nenhuma configura√ß√£o manual

**Rela√ß√£o direta com Lambda**

* Event Source Mapping do Lambda cria **um consumer group gerenciado pela AWS**

**Frase**

> ‚ÄúConsumer groups nos d√£o elasticidade sem coordena√ß√£o manual.‚Äù

---

## 2. Conceitos de particionamento (onde est√° o ouro)

### 2.1 Hash vs Key-based partitioning

#### Hash / Round-robin

* M√°ximo throughput
* Carga bem distribu√≠da
* Ordem global N√ÉO garantida

#### Key-based (ex: c√≥digo 300, 301‚Ä¶)

* Ordem garantida por chave
* Poss√≠vel hotspot

**No seu cen√°rio**

* 80% das mensagens v√™m de poucos c√≥digos
* Se particionar s√≥ pelo c√≥digo ‚Üí risco de gargalo

**Decis√£o madura**

* Usar **composite key**:

  ```text
  key = codigo + hash(deviceId)
  ```

**Frase t√©cnica**

> ‚ÄúMantemos ordem l√≥gica sem sacrificar paralelismo f√≠sico.‚Äù

---

### 2.2 Hot partitions e Lei de Pareto (80/20)

**Conceito**

* 20% das chaves geram 80% do tr√°fego
* Kafka n√£o resolve hotspot sozinho

**Solu√ß√µes arquiteturais**

1. Mais parti√ß√µes no t√≥pico quente
2. T√≥pico dedicado para heavy hitters
3. Sub-particionamento l√≥gico
4. Sharding por tempo ou regi√£o

**Exemplo**

* `mt-c400-heavy`
* `mt-c400-light`

**Frase**

> ‚ÄúIdentificamos hot keys e tratamos como first-class citizens da arquitetura.‚Äù

---

## 3. Conceitos de throughput e performance

### 3.1 Batching (o conceito mais subestimado)

**Conceito**

* Kafka √© otimizado para **lotes**, n√£o mensagens individuais
* Produzir e consumir em batch aumenta throughput em ordens de magnitude

**No consumidor C#**

* `max.poll.records`
* Processar arrays
* Enviar batches ao Lambda

**Frase**

> ‚ÄúN√≥s otimizamos chamadas, n√£o mensagens.‚Äù

---

### 3.2 Backpressure (fundamental para estabilidade)

**Conceito**

* O sistema **precisa saber desacelerar**
* Sem isso ‚Üí colapso em cascata

**No seu pipeline**

```
Kafka ‚Üí C# ‚Üí Lambda ‚Üí Servi√ßos
```

**Estrat√©gias**

* Pausar consumo (`Pause/Resume`)
* Controlar batch size
* Limitar concorr√™ncia do Lambda
* DLQ para falhas

**Frase**

> ‚ÄúPreferimos atrasar processamento a perder dados.‚Äù

---

## 4. Conceitos de sem√¢ntica de entrega

### 4.1 At-least-once vs At-most-once vs Exactly-once

**Kafka por padr√£o**

* At-least-once

**O que isso significa**

* Mensagens podem ser reprocessadas
* Seu downstream precisa ser **idempotente**

**No Lambda**

* Use:

  * Deduplica√ß√£o
  * Chave de idempot√™ncia
  * DynamoDB conditional writes

**Frase**

> ‚ÄúGarantimos consist√™ncia por design, n√£o por sorte.‚Äù

---

## 5. Kafka + Lambda: choque de modelos

### 5.1 Pull (Kafka) vs Push (Lambda)

**Kafka**

* Pull-based
* Controle de ritmo no consumidor

**Lambda**

* Push-based
* Escala agressivamente

**Conflito**

* Lambda pode escalar mais r√°pido que downstream

**Solu√ß√£o**

* Batch controlado
* Concurrency limit
* Step Functions ou SQS buffer se necess√°rio

**Frase**

> ‚ÄúIntegramos dois modelos opostos controlando o ponto de acoplamento.‚Äù

---

## 6. Conceitos de observabilidade (n√≠vel s√™nior)

### 6.1 Lag como SLO

**Conceito**

* Lag = backlog real
* M√©trica mais importante do Kafka

**Voc√™ deve monitorar**

* Lag por parti√ß√£o
* Throughput por consumer
* Tempo Kafka ‚Üí Lambda

**Frase**

> ‚ÄúLag √© nosso term√¥metro operacional.‚Äù

---

### 6.2 Tracing distribu√≠do

**Conceito**

* Uma mensagem cruza:

  * Kafka
  * C#
  * Lambda
  * Servi√ßos

**Solu√ß√£o**

* Correlation ID
* OpenTelemetry
* Logs estruturados

**Frase**

> ‚ÄúObservabilidade n√£o √© opcional em sistemas distribu√≠dos.‚Äù

---

## 7. Conceitos de arquitetura (para fechar com chave de ouro)

### 7.1 Desacoplamento por eventos

**Voc√™ est√° construindo**

* Uma **arquitetura orientada a eventos**
* N√£o RPC, n√£o request/response

**Benef√≠cios**

* Escalabilidade
* Evolu√ß√£o independente
* Resili√™ncia

---

### 7.2 Design para falha

**Assumir**

* Broker cai
* Consumer cai
* Lambda falha

**Kafka permite**

* Replay
* Reten√ß√£o
* Reprocessamento controlado

**Frase final forte**

> ‚ÄúN√£o evitamos falhas, projetamos para sobreviver a elas.‚Äù

---

## Se quiser ir al√©m (n√≠vel doutorado / staff engineer)

Posso te ajudar a:

* Criar **ADR formal** dessas decis√µes
* Calcular **n√∫mero √≥timo de parti√ß√µes**
* Simular throughput em Python
* Criar um **diagrama C4**
* Escrever isso como **cap√≠tulo de tese ou whitepaper**

Perfeito. Abaixo est√° um **diagrama em Mermaid** focado **exatamente nos pontos de tens√£o da discuss√£o arquitetural**:
isolamento vs compartilhamento, blast radius, prioridade, chave de parti√ß√£o e evolu√ß√£o para arquitetura h√≠brida.

A ideia n√£o √© s√≥ mostrar o ‚Äúfluxo feliz‚Äù, mas **visualizar os riscos e as decis√µes**.

---

## 1Ô∏è‚É£ Vis√£o Geral ‚Äì Arquitetura Atual (Dom√≠nio + Parti√ß√µes)

```mermaid
flowchart LR
    subgraph PROD["Producers (.NET em EC2)"]
        P["Producer<br/>- Extrai c√≥digo<br/>- Define key (ex: c√≥digo)<br/>- JSON 200 bytes"]
    end

    subgraph MSK["Amazon MSK"]
        subgraph C400["T√≥pico mt-c400 (mesmo dom√≠nio)"]
            C400_P0["Parti√ß√£o 0<br/>C√≥digo 300"]
            C400_P1["Parti√ß√£o 1<br/>C√≥digo 301"]
            C400_P2["Parti√ß√£o 2<br/>C√≥digo 302"]
            C400_PN["Parti√ß√µes extras<br/>C√≥digos quentes"]
        end
    end

    subgraph CONS["Consumer Group (.NET EC2)"]
        C1["Consumer #1"]
        C2["Consumer #2"]
        C3["Consumer #N"]
    end

    subgraph LAMBDA["AWS Lambda"]
        L["Lambda Dispatcher<br/>- Batch<br/>- Roteia por c√≥digo"]
    end

    P -->|key = c√≥digo| C400
    C400_P0 --> C1
    C400_P1 --> C2
    C400_P2 --> C3
    C400_PN --> C1

    C1 -->|batch| L
    C2 -->|batch| L
    C3 -->|batch| L
```

### üß† O que este diagrama explica

* **Parti√ß√µes s√£o a unidade de escala**, n√£o t√≥picos
* Ordem √© garantida **por c√≥digo**, n√£o global
* Consumer √© gen√©rico ‚Üí reduz filtering cost
* Lambda isola l√≥gica e fan-out

---

## 2Ô∏è‚É£ Onde mora o risco ‚Äì Blast Radius e Hot Partition

```mermaid
flowchart LR
    subgraph MSK["T√≥pico mt-c400"]
        P0["Parti√ß√£o X<br/>C√≥digo 300<br/>(ALTA PRIORIDADE)"]
        P1["Parti√ß√£o Y<br/>C√≥digo 301<br/>(Volume Alto)"]
    end

    subgraph CONS["Consumer EC2"]
        C["Consumer"]
    end

    P1 -->|Mensagem malformada| C
    C -.->|Erro n√£o tratado| P1
    P1 -.->|Lag aumenta| P0

    style P1 fill:#ffdddd
    style P0 fill:#fff4cc
```

### ‚ö†Ô∏è O que este diagrama evidencia

* Um erro em dados **n√£o cr√≠ticos** pode:

  * Aumentar lag
  * Afetar processamento cr√≠tico
* Esse risco **s√≥ existe** se:

  * Consumer tiver l√≥gica pesada
  * Erro travar parti√ß√£o

üëâ **Mitiga√ß√£o**:

* Consumer simples
* DLQ
* Try/catch por mensagem

---

## 3Ô∏è‚É£ Compara√ß√£o visual ‚Äì Muitos T√≥picos vs Poucos T√≥picos

```mermaid
flowchart TB
    subgraph A["Estrat√©gia A<br/>Muitos T√≥picos"]
        A300["topic-code-300"]
        A301["topic-code-301"]
        A302["topic-code-302"]
    end

    subgraph B["Estrat√©gia B<br/>Dom√≠nio + Parti√ß√µes"]
        B400["mt-c400"]
        B400_P["Parti√ß√µes<br/>300 | 301 | 302"]
    end

    style A fill:#ffe6e6
    style B fill:#e6f2ff
```

### üìä Leitura arquitetural

* Estrat√©gia A:

  * Alto isolamento
  * Alto custo operacional
  * Topic sprawl
* Estrat√©gia B:

  * Bom isolamento l√≥gico
  * Escala eficiente
  * Governan√ßa centralizada

---

## 4Ô∏è‚É£ Evolu√ß√£o Natural ‚Äì Arquitetura H√≠brida (Prioridade)

```mermaid
flowchart LR
    subgraph PROD["Producer (.NET)"]
        P["Producer<br/>Decide t√≥pico<br/>com base na prioridade"]
    end

    subgraph MSK["Amazon MSK"]
        HP["high_priority_events<br/>(C√≥digo 300)"]
        STD["telemetry_standard<br/>(301, 302)"]
    end

    subgraph CONS["Consumers EC2"]
        C_HP["Consumers Alta Prioridade"]
        C_STD["Consumers Standard"]
    end

    subgraph LAMBDA["Lambda"]
        L["Dispatcher"]
    end

    P -->|300| HP
    P -->|301/302| STD

    HP --> C_HP
    STD --> C_STD

    C_HP --> L
    C_STD --> L
```

### üß† Mensagem-chave

* Evolu√ß√£o **n√£o quebra o desenho atual**
* Producer j√° controla:

  * Key
  * T√≥pico
* Isolamento cresce conforme criticidade

---

## 5Ô∏è‚É£ Decis√£o sobre chave ‚Äì `truck_id` vs `codigo`

```mermaid
flowchart LR
    subgraph KEY1["key = truck_id"]
        K1["300 + 301<br/>mesma parti√ß√£o"]
    end

    subgraph KEY2["key = c√≥digo"]
        K2["300 ‚Üí parti√ß√£o A"]
        K3["301 ‚Üí parti√ß√£o B"]
    end

    style KEY1 fill:#ffe6e6
    style KEY2 fill:#e6ffe6
```

### üìå Interpreta√ß√£o

* `truck_id`:

  * Ordem forte
  * Alto acoplamento
* `codigo`:

  * Melhor isolamento
  * Ordem suficiente
  * Menor blast radius

---

## 6Ô∏è‚É£ Diagrama mental para defender a arquitetura

```mermaid
flowchart TB
    A["Dom√≠nio bem definido"] --> B["Poucos t√≥picos"]
    B --> C["Parti√ß√µes para escala"]
    C --> D["Producer controla key"]
    D --> E["Consumer simples"]
    E --> F["Lambda isola fan-out"]
    F --> G["Evolu√ß√£o para isolamento por prioridade"]
```

---

## üéØ Como usar esse material na discuss√£o

Voc√™ pode literalmente apontar para os diagramas e dizer:

* ‚ÄúAqui est√° onde assumimos risco conscientemente‚Äù
* ‚ÄúAqui est√° como mitigamos‚Äù
* ‚ÄúAqui est√° como evolu√≠mos sem reescrever tudo‚Äù


*** 

# Abordagem do producer 

1. Objetivo da POC
2. Responsabilidades do Producer
3. Arquitetura l√≥gica do Producer
4. Estrat√©gia para t√≥picos e parti√ß√µes
5. Escala: 1 inst√¢ncia vs m√∫ltiplas inst√¢ncias
6. Controle, observabilidade e governan√ßa
7. Fluxograma (Mermaid)
8. Etapas de implementa√ß√£o
9. Pontos perigosos e riscos reais

---

# üéØ 1. Objetivo da POC (Producer)

### O que a POC PRECISA provar

A POC do **Producer EC2 .NET** deve responder objetivamente:

* Consigo **publicar corretamente** em m√∫ltiplos t√≥picos?
* Consigo **controlar a distribui√ß√£o de mensagens por parti√ß√£o**?
* Consigo **simular volume realista**?
* Consigo **escalar horizontalmente sem quebrar nada**?
* Consigo **medir impacto em lat√™ncia e custo**?

‚ö†Ô∏è **Importante**:
A POC **n√£o** precisa:

* Ser altamente resiliente
* Ter autoscaling perfeito
* Ter HA completo

Ela precisa **validar decis√µes arquiteturais**, n√£o ser produ√ß√£o.

---

# üß± 2. Responsabilidades do Producer

O Producer **n√£o √© burro**, mas tamb√©m **n√£o deve ser inteligente demais**.

### Responsabilidades corretas

‚úî Escolher o t√≥pico
‚úî Definir a key de parti√ß√£o
‚úî Garantir confiabilidade (`acks=all`)
‚úî Controlar taxa (throttling)
‚úî Emitir m√©tricas

### Responsabilidades que N√ÉO s√£o dele

‚ùå Saber quantas parti√ß√µes existem
‚ùå Fazer load balancing manual
‚ùå Garantir ordem global
‚ùå Saber quem vai consumir

üëâ **Kafka j√° faz isso melhor.**

---

# üß† 3. Arquitetura l√≥gica do Producer

Pense no Producer como **pipeline**, n√£o como um ‚Äúservi√ßo REST‚Äù.

```
Input Generator
     ‚Üì
Message Builder
     ‚Üì
Topic Resolver
     ‚Üì
Partition Key Resolver
     ‚Üì
Kafka Producer
     ‚Üì
Metrics & Logs
```

---

# üß© 4. Estrat√©gia para t√≥picos e parti√ß√µes

## üîë Regra de ouro

> **Producer escolhe a KEY, Kafka escolhe a PARTI√á√ÉO**

### Exemplo

```text
Topic: mt-c400
Key: hash(codigo + truck_id)
```

Kafka:

* Aplica hash(key)
* Mod N (parti√ß√µes)
* Distribui automaticamente

‚ö†Ô∏è Voc√™ **n√£o** deve usar `partition=X` manualmente na POC.

---

## üîç Estrat√©gia de chave (decis√£o cr√≠tica)

### Op√ß√µes

| Estrat√©gia           | Pr√≥s               | Contras        |
| -------------------- | ------------------ | -------------- |
| `truck_id`           | Ordem por caminh√£o | Hot partitions |
| `codigo`             | Isolamento l√≥gico  | Perda de ordem |
| `hash(codigo+truck)` | Equil√≠brio         | Ordem parcial  |

üëâ **Para POC:**
Use `hash(codigo + truck_id)`
√â a mais defens√°vel.

---

# ‚öñÔ∏è 5. Escala: quantas inst√¢ncias de Producer?

### ‚ùó Verdade importante

> **Kafka Producer escala melhor por THREAD do que por INST√ÇNCIA**

### Estrat√©gia correta para POC

#### Fase 1 ‚Äì 1 EC2

* 1 processo
* 1 Producer
* 5‚Äì10 threads
* Controle de TPS

#### Fase 2 ‚Äì 2 EC2

* Mesma config
* Mesmos t√≥picos
* Mesmas chaves

üëâ Kafka garante que:

* N√£o h√° duplica√ß√£o
* N√£o h√° conflito
* N√£o h√° desordem por key

---

## ‚ö†Ô∏è Erro comum

> ‚ÄúVou criar um Producer por parti√ß√£o‚Äù

‚ùå ERRADO
Isso quebra:

* Escala
* Rebalance
* Custo

---

# üéõÔ∏è 6. Controle e governan√ßa

### Controle de taxa (obrigat√≥rio)

```text
- Mensagens/segundo
- Bytes/segundo
```

Implemente:

* Token bucket simples
* Sleep controlado
* Config via ENV VAR

---

### Observabilidade m√≠nima

| M√©trica     | Por qu√™        |
| ----------- | -------------- |
| msg/s       | Throughput     |
| ack latency | Sa√∫de MSK      |
| error rate  | Confiabilidade |
| retries     | Satura√ß√£o      |

---

# üîÅ 7. Fluxograma do Producer (Mermaid)

```mermaid
flowchart TD
    A[Start Producer] --> B[Load Config]
    B --> C[Initialize Kafka Producer]
    C --> D[Start Worker Threads]

    D --> E[Generate Payload]
    E --> F[Resolve Topic]
    F --> G[Resolve Key]
    G --> H[Send to Kafka]

    H --> I{Ack OK?}
    I -->|Yes| J[Emit Metrics]
    I -->|No| K[Retry / Log Error]

    J --> E
```

---

# üõ†Ô∏è 8. Etapas pr√°ticas da POC

## Etapa 1 ‚Äì Infra m√≠nima

* EC2 t3.medium
* Security Group liberando MSK
* Certificados funcionando

---

## Etapa 2 ‚Äì C√≥digo Producer .NET

Componentes:

* `ProducerFactory`
* `MessageGenerator`
* `TopicRouter`
* `KeyResolver`
* `MetricsEmitter`

---

## Etapa 3 ‚Äì Simula√ß√£o de carga

* Ramp-up gradual
* 1 ‚Üí 5 ‚Üí 10 threads
* Observa lat√™ncia

---

## Etapa 4 ‚Äì Scale-out

* Sobe segunda EC2
* Mesmo c√≥digo
* Observa:

  * Distribui√ß√£o
  * Throughput
  * Impacto no MSK

---

# ‚ò†Ô∏è 9. Pontos perigosos e riscos reais

## üî• Risco 1 ‚Äì Hot partitions

* Chave mal escolhida
* Solu√ß√£o: hash composta

---

## üî• Risco 2 ‚Äì Buffer interno do Producer

* `queue.buffering.max.messages`
* Pode causar OOM
* Monitorar

---

## üî• Risco 3 ‚Äì Backpressure invis√≠vel

* MSK lento
* Producer continua aceitando
* Lat√™ncia explode

---

## üî• Risco 4 ‚Äì Overengineering

* Producer ‚Äúinteligente demais‚Äù
* L√≥gica duplicada
* Dif√≠cil de manter

---

## üî• Risco 5 ‚Äì Falta de m√©tricas

> ‚ÄúSem m√©tricas, voc√™ n√£o tem arquitetura, s√≥ esperan√ßa.‚Äù

---

# üß† Conclus√£o arquitetural

> O Producer **n√£o controla o Kafka**.
> Ele **declara inten√ß√£o** (key, t√≥pico, confiabilidade).
> O Kafka **executa a estrat√©gia de escala**.

Essa POC vai te permitir:

* Defender a arquitetura com n√∫meros
* Antecipar gargalos
* Calibrar decis√µes antes da produ√ß√£o

---

## üîú Pr√≥ximo passo (se quiser)

Posso:

* Criar **pseudoc√≥digo .NET detalhado do Producer**
* Criar **checklist de POC**
* Criar **template de relat√≥rio de resultados**
* Avan√ßar para **EC2 Consumer**

Se quiser, diga:
üëâ *‚ÄúVamos detalhar o c√≥digo do Producer‚Äù*

Perfeito. Vou te entregar **dois diagramas complementares**, no n√≠vel que normalmente aparece em **documenta√ß√£o de arquitetura s√©ria**:

1. **Diagrama arquitetural do Producer (vis√£o estrutural)**
2. **Fluxograma detalhado de execu√ß√£o do Producer (vis√£o comportamental)**

Ambos pensados para:

* Sustentar discuss√£o com arquitetos, SRE e FinOps
* Ajudar algu√©m a **construir o servi√ßo do zero**
* Deixar expl√≠cito **onde est√£o os riscos e controles**

---

# üß± 1Ô∏è‚É£ Diagrama Arquitetural do Servi√ßo Producer (.NET em EC2)

üëâ **Pergunta que esse diagrama responde**

> ‚ÄúQue componentes existem dentro do Producer e como eles se relacionam?‚Äù

```mermaid
flowchart LR
    subgraph EC2["EC2 Producer (.NET)"]
        CFG["Config Loader<br/>(ENV / AppSettings)"]
        GEN["Message Generator<br/>(Simula fontes)"]
        ROUTER["Topic Router<br/>(Regra por c√≥digo)"]
        KEY["Key Resolver<br/>(hash(codigo+truck))"]
        KPROD["Kafka Producer<br/>(librdkafka)"]
        MET["Metrics & Logs<br/>(CloudWatch / OTEL)"]
        RATE["Rate Limiter<br/>(TPS / Bytes)"]
    end

    subgraph MSK["Amazon MSK"]
        T400["Topic mt-c400<br/>(N Parti√ß√µes)"]
        T300["Topic mt-c300<br/>(M Parti√ß√µes)"]
    end

    CFG --> GEN
    GEN --> ROUTER
    ROUTER --> KEY
    KEY --> RATE
    RATE --> KPROD
    KPROD --> T400
    KPROD --> T300
    KPROD --> MET
    RATE --> MET
```

---

## üìå Como explicar esse diagrama em reuni√£o

* **Config Loader**
  ‚Üí Nada hardcoded. TPS, t√≥picos e comportamento s√£o configur√°veis.

* **Message Generator**
  ‚Üí Simula m√∫ltiplas fontes e Pareto (80/20).

* **Topic Router**
  ‚Üí Decide *qual dom√≠nio* (mt-c400, mt-c300).

* **Key Resolver**
  ‚Üí Define *como Kafka ir√° distribuir*.

* **Rate Limiter**
  ‚Üí Protege o MSK **e o pr√≥prio Producer**.

* **Kafka Producer**
  ‚Üí √önico ponto de comunica√ß√£o com o cluster.

* **Metrics**
  ‚Üí Sem isso, a POC n√£o vale nada.

---

# üîÅ 2Ô∏è‚É£ Fluxograma Detalhado do Producer (Execu√ß√£o passo a passo)

üëâ **Pergunta que esse fluxograma responde**

> ‚ÄúO que acontece exatamente desde o start at√© o envio cont√≠nuo?‚Äù

```mermaid
flowchart TD
    A[Start EC2 Producer] --> B[Load Configurations]
    B --> C[Validate Configs]
    C -->|Invalid| CERR[Fail Fast & Exit]
    C -->|Valid| D[Init Kafka Producer]

    D --> E[Init Metrics & Logs]
    E --> F[Start Worker Threads]

    F --> G[Generate Message]
    G --> H[Extract Business Code]
    H --> I[Resolve Topic]
    I --> J[Resolve Partition Key]

    J --> K[Rate Limit Check]
    K -->|Exceeded| KWAIT[Wait / Sleep]
    KWAIT --> K
    K -->|Allowed| L[Produce Message]

    L --> M{Ack Received?}
    M -->|Yes| N[Update Metrics]
    M -->|No| O[Retry Policy]

    O -->|Retryable| L
    O -->|Fatal| P[Log Error & Continue]

    N --> G
```

---

## üìå Pontos cr√≠ticos destacados no fluxograma

### üî¥ Fail Fast

* Se config errada ‚Üí **n√£o sobe**
* Evita producer ‚Äúzumbi‚Äù

---

### üî¥ Rate Limiter antes do Kafka

* Kafka **n√£o protege voc√™**
* Sem isso:

  * Mem√≥ria cresce
  * Lat√™ncia explode
  * EC2 morre

---

### üî¥ Retry consciente

* Nem todo erro √© retry√°vel
* Erro de schema ‚â† erro de rede

---

# ‚ö†Ô∏è Pontos perigosos expl√≠citos no diagrama

| Ponto             | Risco              | Mitiga√ß√£o         |
| ----------------- | ------------------ | ----------------- |
| Key mal escolhida | Hot partition      | Hash composta     |
| Sem rate limit    | OOM                | Throttling        |
| Retries infinitos | Loop infinito      | Retry com limite  |
| Config fixa       | Imposs√≠vel escalar | ENV vars          |
| M√©tricas pobres   | Arquitetura ‚Äúcega‚Äù | CloudWatch / OTEL |

---

# üß† Como um arquiteto s√™nior justificaria esse desenho

> ‚ÄúEsse Producer n√£o sabe nada sobre o MSK al√©m do necess√°rio. Ele declara inten√ß√£o (t√≥pico, chave, confiabilidade) e deixa o Kafka fazer o trabalho pesado. Isso reduz acoplamento, facilita escala e torna o comportamento previs√≠vel sob carga.‚Äù

---

# üìã Checklist r√°pido (para POC)

‚úî C√≥digo configur√°vel
‚úî Simula√ß√£o Pareto
‚úî M√©tricas claras
‚úî Rate limit ativo
‚úî Chave bem definida
‚úî F√°cil de escalar horizontalmente

Perfeito. Abaixo est√° um **PLAYBOOK DE AUMENTO DE PARTI√á√ïES KAFKA (MSK)**, escrito no n√≠vel que **SRE, arquiteto e time .NET** esperam ver.
Ele cobre **quando fazer**, **impactos reais**, **o que esperar**, **como mitigar**, e **como o c√≥digo .NET deve se comportar antes e depois**.

Este material √© **diretamente reutiliz√°vel** na sua documenta√ß√£o oficial.

---

# üìò PLAYBOOK ‚Äì Aumento de Parti√ß√µes em Kafka (MSK)

## üéØ Objetivo

Permitir **aumento de throughput e paralelismo** sem downtime, mantendo:

* previsibilidade operacional
* impacto controlado
* zero mudan√ßa no c√≥digo do Producer/Consumer (.NET)

---

## 1Ô∏è‚É£ Quando aumentar parti√ß√µes (crit√©rios objetivos)

Voc√™ **N√ÉO aumenta parti√ß√µes por intui√ß√£o**.
Voc√™ aumenta quando **m√©tricas indicam gargalo estrutural**.

### Indicadores claros

| M√©trica                            | Sintoma                 |
| ---------------------------------- | ----------------------- |
| Consumer Lag cresce continuamente  | Capacidade insuficiente |
| Apenas algumas parti√ß√µes saturadas | Hot partition           |
| CPU de consumers < 50%             | Falta paralelismo       |
| Throughput estagnado               | Limite f√≠sico           |
| Escalar consumers n√£o ajuda        | Falta parti√ß√µes         |

üëâ **Regra pr√°tica**

> N√∫mero de parti√ß√µes ‚â• n√∫mero m√°ximo esperado de consumers ativos

---

## 2Ô∏è‚É£ O que acontece tecnicamente ao aumentar parti√ß√µes

### Estado ANTES

```
Topic mt-c400
Parti√ß√µes: 8
partition = hash(key) % 8
```

### Estado DEPOIS

```
Topic mt-c400
Parti√ß√µes: 16
partition = hash(key) % 16
```

### Efeitos imediatos

‚úî Mensagens antigas permanecem onde est√£o
‚úî Mensagens novas passam a usar novas parti√ß√µes
‚ö† A mesma key pode ir para outra parti√ß√£o
‚ö† Ordem hist√≥rica entre mensagens antigas e novas √© quebrada

üëâ **Nada √© perdido. Nada √© duplicado.**

---

## 3Ô∏è‚É£ Impactos esperados (e normais)

### 3.1 Rebalance do Consumer Group

**O que acontece**

* Todos os consumers pausam
* Kafka redistribui parti√ß√µes
* Consumo retoma

**O que esperar**

* Pausa de segundos (ou minutos se exagerou nas parti√ß√µes)
* Lag tempor√°rio

**Mitiga√ß√£o**

* Aumentar parti√ß√µes fora do hor√°rio de pico
* Garantir que consumers sejam idempotentes

---

### 3.2 Redistribui√ß√£o de carga

**Antes**

```
Parti√ß√£o 2 ‚Üí 80% do tr√°fego
```

**Depois**

```
Parti√ß√£o 2 ‚Üí 40%
Parti√ß√£o 10 ‚Üí 40%
```

üëâ **Esse √© o ganho esperado.**

---

### 3.3 Mudan√ßa invis√≠vel para o Producer

‚úî Producer n√£o sabe
‚úî Producer n√£o muda
‚úî Producer continua saud√°vel

**SE** voc√™ usou:

* key-based partitioning
* sem parti√ß√£o fixa no c√≥digo

---

## 4Ô∏è‚É£ Riscos reais e como resolver

### üî• RISCO 1 ‚Äì Explos√£o de rebalance

**Causa**

* Muitas parti√ß√µes
* Muitos consumers
* Deploy frequente

**Mitiga√ß√£o**

* Evitar ‚Äúparti√ß√µes demais‚Äù
* Preferir scale-up antes de scale-out
* Evitar restart em massa

---

### üî• RISCO 2 ‚Äì Quebra de expectativa de ordem

**Causa**

* Key vai para outra parti√ß√£o

**Mitiga√ß√£o**

* Documentar claramente:

  > ‚ÄúOrdem √© garantida apenas ap√≥s o resize‚Äù
* Usar timestamp de evento

---

### üî• RISCO 3 ‚Äì Hot partition continua

**Causa**

* Chave mal escolhida

**Mitiga√ß√£o**

```text
ANTES: key = codigo
DEPOIS: key = hash(codigo + truck_id)
```

---

## 5Ô∏è‚É£ Como se preparar ANTES do aumento (obrigat√≥rio)

### ‚úî 1. C√≥digo .NET deve ser agn√≥stico a parti√ß√µes

‚ùå ERRADO

```csharp
new TopicPartition("mt-c400", new Partition(3))
```

‚úî CORRETO

```csharp
producer.Produce(
    "mt-c400",
    new Message<string, string>
    {
        Key = $"{codigo}:{truckId}",
        Value = payload
    }
);
```

---

### ‚úî 2. Consumers devem ser idempotentes

Ap√≥s rebalance:

* Mensagem pode ser reprocessada
* Offset pode ser replayado

üëâ Lambda downstream deve suportar isso.

---

### ‚úî 3. M√©tricas obrigat√≥rias antes do resize

* Consumer lag por parti√ß√£o
* Rebalance time
* Throughput por key
* Ack latency do Producer

---

## 6Ô∏è‚É£ Passo a passo operacional (Playbook)

### üîπ PASSO 1 ‚Äì Validar necessidade

* Confirmar gargalo
* Confirmar que mais consumers n√£o resolvem

---

### üîπ PASSO 2 ‚Äì Escolher novo n√∫mero

Regra pr√°tica:

```
Novo N = 2x parti√ß√µes atuais
```

Evite:

* Incrementos pequenos
* Ajustes frequentes

---

### üîπ PASSO 3 ‚Äì Executar resize

```bash
kafka-topics.sh \
  --bootstrap-server <broker> \
  --alter \
  --topic mt-c400 \
  --partitions 16
```

‚úî Sem downtime
‚úî Sem restart

---

### üîπ PASSO 4 ‚Äì Monitorar rebalance

Acompanhar:

* consumer-group-state
* lag total
* throughput

---

### üîπ PASSO 5 ‚Äì Validar estabiliza√ß√£o

Crit√©rio de sucesso:

* Lag volta ao normal
* CPU de consumers sobe
* Throughput aumenta

---

## 7Ô∏è‚É£ O que N√ÉO fazer (anti-patterns)

‚ùå Fixar parti√ß√µes no Producer
‚ùå Criar l√≥gica ‚Äúparti√ß√£o dedicada‚Äù no c√≥digo
‚ùå Aumentar parti√ß√µes toda semana
‚ùå Ignorar rebalance
‚ùå Achar que aumento √© ‚Äúsem impacto‚Äù

---

## 8Ô∏è‚É£ Pergunta dura de SRE (simula√ß√£o)

### ‚ùì ‚ÄúSe dobrarmos as parti√ß√µes agora, o que pode dar errado?‚Äù

**Resposta correta**

> ‚ÄúTeremos um rebalance tempor√°rio e poss√≠vel quebra de ordem hist√≥rica, mas nenhum impacto em integridade, perda de dados ou necessidade de mudan√ßa de c√≥digo.‚Äù

---

## 9Ô∏è‚É£ Pergunta dura de arquiteto (simula√ß√£o)

### ‚ùì ‚ÄúPor que n√£o criar outro t√≥pico em vez de aumentar parti√ß√µes?‚Äù

**Resposta madura**

> ‚ÄúPorque o dom√≠nio n√£o mudou. S√≥ a capacidade. Parti√ß√µes escalam processamento, t√≥picos escalam contratos.‚Äù

---

## üîö Conclus√£o para documenta√ß√£o

> ‚ÄúAumentar parti√ß√µes √© uma opera√ß√£o planejada, previs√≠vel e segura quando o Producer √© key-based, os Consumers s√£o idempotentes e o impacto √© monitorado. O risco n√£o est√° no Kafka, est√° em arquiteturas que acoplam c√≥digo √† topologia.‚Äù


